import requests
import json
import hashlib
import logging
import logging.handlers
from datetime import datetime
from typing import Dict, Any, List, Optional
from pathlib import Path
import time
import re
import pika


class HHVacancyParser:
    def __init__(self, log_dir: str = "../logs", rabbitmq_host: str = 'localhost',
                 queue_name: str = 'json_processing_queue'):
        self.base_url = "https://api.hh.ru/vacancies"
        self.headers = {"User-Agent": "CDEK HR Analytics/1.0"}
        self.output_dir = None
        self.chunk_size = 10
        self.saved_files = []
        self.parsed_data = []

        # RabbitMQ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
        self.rabbitmq_host = rabbitmq_host
        self.queue_name = queue_name
        self.connection = None
        self.channel = None
        self.rabbitmq_connected = False

        self.logger = logging.getLogger("HHVacancyParser")
        self.logger.setLevel(logging.DEBUG)

        formatter = logging.Formatter(
            "%(asctime)s - %(name)s - %(levelname)s - %(message)s",
            datefmt="%Y-%m-%d %H:%M:%S"
        )

        console_handler = logging.StreamHandler()
        console_handler.setLevel(logging.INFO)
        console_handler.setFormatter(formatter)
        self.logger.addHandler(console_handler)

        # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è –ª–æ–≥–æ–≤
        log_path = Path(__file__).parent.parent.parent / log_dir
        log_path.mkdir(exist_ok=True)

        file_handler = logging.handlers.RotatingFileHandler(
            filename=str(log_path / "hh_parser.log"),
            maxBytes=1_000_000,
            backupCount=5
        )
        file_handler.setLevel(logging.DEBUG)
        file_handler.setFormatter(formatter)
        self.logger.addHandler(file_handler)

    def connect_rabbitmq(self):
        """–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ RabbitMQ"""
        try:
            self.connection = pika.BlockingConnection(
                pika.ConnectionParameters(host=self.rabbitmq_host)
            )
            self.channel = self.connection.channel()
            self.channel.queue_declare(queue=self.queue_name, durable=True)
            self.rabbitmq_connected = True
            self.logger.info(f"–ü–æ–¥–∫–ª—é—á–µ–Ω–æ –∫ RabbitMQ –Ω–∞ {self.rabbitmq_host}")
            return True
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ RabbitMQ: {e}")
            return False

    def send_to_rabbitmq(self, data: List[Dict]):
        """–û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –¥–∞–Ω–Ω—ã–µ –≤ RabbitMQ"""
        if not self.rabbitmq_connected:
            self.logger.warning("RabbitMQ –Ω–µ –ø–æ–¥–∫–ª—é—á–µ–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –æ—Ç–ø—Ä–∞–≤–∫—É")
            return False

        try:
            json_data = json.dumps(data, ensure_ascii=False, indent=2)
            self.channel.basic_publish(
                exchange='',
                routing_key=self.queue_name,
                body=json_data,
                properties=pika.BasicProperties(
                    delivery_mode=2,
                    content_type='application/json'
                )
            )
            self.logger.info(f"üì§ –û—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ –≤ RabbitMQ: {len(data)} –≤–∞–∫–∞–Ω—Å–∏–π")
            return True
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ –≤ RabbitMQ: {e}")
            return False

    def close_rabbitmq(self):
        """–ó–∞–∫—Ä—ã–≤–∞–µ—Ç —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Å RabbitMQ"""
        if self.connection and not self.connection.is_closed:
            self.connection.close()
            self.logger.info("üîå –°–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Å RabbitMQ –∑–∞–∫—Ä—ã—Ç–æ")

    def init_output_dir(self, time_str: str):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø–∞–ø–∫—É –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        output_path = Path(__file__).parent.parent.parent / "data" / "output" / f"result_{time_str}"
        output_path.mkdir(parents=True, exist_ok=True)
        self.output_dir = output_path
        self.logger.info(f"–°–æ–∑–¥–∞–Ω–∞ –ø–∞–ø–∫–∞ –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {self.output_dir}")

    def fetch_vacancies(self, params: Dict, retries: int = 3, delay: float = 1.0) -> List[Dict]:
        """–ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –≤–∞–∫–∞–Ω—Å–∏–π —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏"""
        for attempt in range(retries):
            try:
                response = requests.get(self.base_url, params=params, headers=self.headers)
                response.raise_for_status()
                data = response.json()
                self.logger.info(f"–ü–æ–ª—É—á–µ–Ω–æ {len(data.get('items', []))} –≤–∞–∫–∞–Ω—Å–∏–π –¥–ª—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ {params}")
                return data.get("items", [])
            except requests.exceptions.HTTPError as e:
                if response.status_code == 403:
                    self.logger.error(f"403 Forbidden –¥–ª—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ {params}: {e}")
                    return []
                elif response.status_code == 429:
                    self.logger.warning(f"429 Rate Limit, –ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{retries}, –∂–¥—É {delay}s")
                    time.sleep(delay * (2 ** attempt))
                else:
                    self.logger.error(f"HTTP –æ—à–∏–±–∫–∞ –¥–ª—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ {params}: {e}")
            except requests.exceptions.RequestException as e:
                self.logger.error(f"–°–µ—Ç–µ–≤–∞—è –æ—à–∏–±–∫–∞ –¥–ª—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ {params}: {e}")
                time.sleep(delay)
        self.logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –≤–∞–∫–∞–Ω—Å–∏–∏ –¥–ª—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ {params} –ø–æ—Å–ª–µ {retries} –ø–æ–ø—ã—Ç–æ–∫")
        return []

    def get_vacancy_details(self, vacancy_id: str, retries: int = 3, delay: float = 1.0) -> Optional[Dict[str, Any]]:
        """–ü–æ–ª—É—á–∞–µ—Ç –ø–æ–ª–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –ø–æ –≤–∞–∫–∞–Ω—Å–∏–∏ —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏"""
        self.logger.debug(f"–ó–∞–ø—Ä–æ—Å –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤–∞–∫–∞–Ω—Å–∏–∏ {vacancy_id}")
        for attempt in range(retries):
            try:
                response = requests.get(f"{self.base_url}/{vacancy_id}", headers=self.headers)
                response.raise_for_status()
                self.logger.info(f"–£—Å–ø–µ—à–Ω–æ –ø–æ–ª—É—á–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≤–∞–∫–∞–Ω—Å–∏–∏ {vacancy_id}")
                return response.json()
            except requests.exceptions.HTTPError as e:
                if response.status_code == 403:
                    self.logger.error(f"403 Forbidden –¥–ª—è –≤–∞–∫–∞–Ω—Å–∏–∏ {vacancy_id}: {e}")
                    return None
                elif response.status_code == 429:
                    self.logger.warning(
                        f"429 Rate Limit –¥–ª—è –≤–∞–∫–∞–Ω—Å–∏–∏ {vacancy_id}, –ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{retries}, –∂–¥—É {delay}s")
                    time.sleep(delay * (2 ** attempt))
                else:
                    self.logger.error(f"HTTP –æ—à–∏–±–∫–∞ –¥–ª—è –≤–∞–∫–∞–Ω—Å–∏–∏ {vacancy_id}: {e}")
            except requests.exceptions.RequestException as e:
                self.logger.error(f"–°–µ—Ç–µ–≤–∞—è –æ—à–∏–±–∫–∞ –¥–ª—è –≤–∞–∫–∞–Ω—Å–∏–∏ {vacancy_id}: {e}")
                time.sleep(delay)
        self.logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≤–∞–∫–∞–Ω—Å–∏–∏ {vacancy_id} –ø–æ—Å–ª–µ {retries} –ø–æ–ø—ã—Ç–æ–∫")
        return None

    def normalize_salary(self, salary_data: Optional[Dict]) -> Dict:
        """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –¥–∞–Ω–Ω—ã–µ –æ –∑–∞—Ä–ø–ª–∞—Ç–µ, –æ–∫—Ä—É–≥–ª—è—è –º–µ–¥–∏–∞–Ω–Ω—É—é –∑–∞—Ä–ø–ª–∞—Ç—É –¥–æ —Ü–µ–ª–æ–≥–æ"""
        if not salary_data:
            self.logger.warning("–î–∞–Ω–Ω—ã–µ –æ –∑–∞—Ä–ø–ª–∞—Ç–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç, —Å–æ–∑–¥–∞–µ—Ç—Å—è –∫–æ–º–ø–µ–Ω—Å–∞—Ü–∏—è —Å –ø—É—Å—Ç—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏")
            compensation_str = "no_salary:default"
            compensation_id = hashlib.md5(compensation_str.encode("utf-8")).hexdigest()
            return {
                "id": compensation_id,
                "salary_min": None,
                "salary_max": None,
                "salary_median": None,
                "salary_avg": None,
                "salary_net": None,
                "currency": None,
                "bonuses": "",
                "payment_frequency": "",
                "payment_type": ""
            }

        gross = salary_data.get("gross", False)
        salary_from = salary_data.get("from")
        salary_to = salary_data.get("to")
        currency = salary_data.get("currency", "RUR")

        avg_salary = None
        if salary_from and salary_to:
            avg_salary = round((salary_from + salary_to) / 2)
        elif salary_from:
            avg_salary = round(salary_from * 1.15)
        elif salary_to:
            avg_salary = round(salary_to * 0.85)  # Assume median is 85% of max if only max is provided

        # Handle None values in compensation_str to ensure consistent hashing
        salary_from_str = str(salary_from) if salary_from is not None else "none"
        salary_to_str = str(salary_to) if salary_to is not None else "none"
        currency_str = str(currency) if currency is not None else "none"
        compensation_str = f"{salary_from_str}:{salary_to_str}:{currency_str}"
        compensation_id = hashlib.md5(compensation_str.encode("utf-8")).hexdigest()

        self.logger.debug(
            f"–ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–∞ –∑–∞—Ä–ø–ª–∞—Ç–∞: min={salary_from}, max={salary_to}, median={avg_salary}, currency={currency}, id={compensation_id}")
        return {
            "id": compensation_id,
            "salary_min": salary_from,
            "salary_max": salary_to,
            "salary_median": avg_salary,
            "salary_avg": avg_salary,
            "salary_net": not gross,
            "currency": currency,
            "bonuses": "",
            "payment_frequency": "monthly",
            "payment_type": "fixed"
        }


    def parse_benefits(self, description: str) -> Dict:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –æ–ø–∏—Å–∞–Ω–∏–µ –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç –±–µ–Ω–µ—Ñ–∏—Ç—ã"""
        benefits = {
            "health_insurance": False,
            "fuel_compensation": False,
            "mobile_compensation": False,
            "free_meals": False,
            "other_benefits": [],
            "new_column": False
        }

        desc_lower = description.lower()
        if "–¥–º—Å" in desc_lower or "–º–µ–¥–∏—Ü–∏–Ω—Å–∫–∞—è —Å—Ç—Ä–∞—Ö–æ–≤–∫–∞" in desc_lower:
            benefits["health_insurance"] = True
        if "–≥—Å–º" in desc_lower or "—Ç–æ–ø–ª–∏–≤–æ" in desc_lower:
            benefits["fuel_compensation"] = True
        if "—Å–≤—è–∑—å" in desc_lower and ("–æ–ø–ª–∞—Ç–∞" in desc_lower or "–∫–æ–º–ø–µ–Ω—Å–∞—Ü–∏—è" in desc_lower):
            benefits["mobile_compensation"] = True
        if "–ø–∏—Ç–∞–Ω–∏–µ" in desc_lower and ("–æ–ø–ª–∞—Ç–∞" in desc_lower or "–±–µ—Å–ø–ª–∞—Ç–Ω–æ–µ" in desc_lower):
            benefits["free_meals"] = True

        benefits_str = f"{benefits['health_insurance']}:{benefits['fuel_compensation']}:{benefits['mobile_compensation']}:{benefits['free_meals']}"
        benefits["id"] = hashlib.md5(benefits_str.encode("utf-8")).hexdigest()

        self.logger.debug(f"–ò–∑–≤–ª–µ—á–µ–Ω—ã –±–µ–Ω–µ—Ñ–∏—Ç—ã: {benefits}")
        return benefits

    def normalize_company_name(self, name: str) -> str:
        """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏"""
        name = name.lower().replace("–æ–æ–æ", "").replace("–∑–∞–æ", "").replace("–∞–æ", "").strip()
        self.logger.debug(f"–ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–æ –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏: {name}")
        return name

    def generate_company_id(self, name: str) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã–π ID –∫–æ–º–ø–∞–Ω–∏–∏"""
        normalized_name = self.normalize_company_name(name)
        company_id = hashlib.md5(normalized_name.encode("utf-8")).hexdigest()
        self.logger.debug(f"–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω company_id: {company_id} –¥–ª—è {name}")
        return company_id

    def extract_experience_from_description(self, description: str, experience: Optional[Dict]) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ –æ–ø—ã—Ç—É –∏–∑ –æ–ø–∏—Å–∞–Ω–∏—è –∏ –¥–∞–Ω–Ω—ã—Ö API"""
        if experience and experience.get("name"):
            return experience["name"]

        desc_lower = description.lower()
        experience_patterns = [
            r'–æ–ø—ã—Ç[–∞-—è\s]*(\d+)[\s-]*–ª–µ—Ç',
            r'–æ—Ç\s*(\d+)\s*–ª–µ—Ç',
            r'(\d+)[\s+-]*–≥–æ–¥–∞?\s*–æ–ø—ã—Ç',
            r'experience[:\s]*(\d+)[\s-]*years?'
        ]

        for pattern in experience_patterns:
            match = re.search(pattern, desc_lower)
            if match:
                years = match.group(1)
                return f"–û—Ç {years} –ª–µ—Ç"

        return "–ù–µ —É–∫–∞–∑–∞–Ω"

    def determine_company_size(self, employer_data: Dict) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ä–∞–∑–º–µ—Ä –∫–æ–º–ø–∞–Ω–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        return "–ù–µ —É–∫–∞–∑–∞–Ω"

    def parse_vacancy(self, vacancy_data: Dict, require_salary: bool = True) -> Optional[Dict[str, Any]]:
        """–ü–∞—Ä—Å–∏—Ç –¥–∞–Ω–Ω—ã–µ –≤–∞–∫–∞–Ω—Å–∏–∏ –≤ —Ñ–æ—Ä–º–∞—Ç –¥–ª—è –ë–î"""
        self.logger.info(f"–ü–∞—Ä—Å–∏–Ω–≥ –≤–∞–∫–∞–Ω—Å–∏–∏ {vacancy_data.get('id')}")

        description = vacancy_data.get("description", "").strip()
        if not description:
            self.logger.warning(f"–í–∞–∫–∞–Ω—Å–∏—è {vacancy_data.get('id')} –ø—Ä–æ–ø—É—â–µ–Ω–∞: –ø—É—Å—Ç–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ")
            return None

        salary_info = self.normalize_salary(vacancy_data.get("salary"))

        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –≤–∞–∫–∞–Ω—Å–∏–∏ –±–µ–∑ –¥–∞–Ω–Ω—ã—Ö –æ –∑–∞—Ä–ø–ª–∞—Ç–µ
        if salary_info["salary_min"] is None and salary_info["salary_max"] is None:
            self.logger.warning(f"–í–∞–∫–∞–Ω—Å–∏—è {vacancy_data.get('id')} –ø—Ä–æ–ø—É—â–µ–Ω–∞: –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –¥–∞–Ω–Ω—ã–µ –æ –∑–∞—Ä–ø–ª–∞—Ç–µ")
            return None

        benefits = self.parse_benefits(description)
        publication_date = vacancy_data.get("published_at")
        if publication_date:
            try:
                publication_date = datetime.strptime(publication_date, "%Y-%m-%dT%H:%M:%S%z").date().isoformat()
            except ValueError as e:
                self.logger.error(f"–û—à–∏–±–∫–∞ —Ñ–æ—Ä–º–∞—Ç–∞ –¥–∞—Ç—ã –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –¥–ª—è –≤–∞–∫–∞–Ω—Å–∏–∏ {vacancy_data.get('id')}: {e}")
                publication_date = None

        employer_data = vacancy_data.get("employer", {})
        company_name = employer_data.get("name") or "–ù–µ —É–∫–∞–∑–∞–Ω"
        company_id = self.generate_company_id(company_name)

        name_variations = [self.normalize_company_name(company_name)]

        return {
            "vacancies": {
                "external_id": vacancy_data.get("id"),
                "similar_titles": [],
                "title": vacancy_data.get("name"),
                "exclude_keywords": [],
                "description": description,
                "requirements": vacancy_data.get("snippet", {}).get("requirement") or "",
                "work_format": self.detect_work_format(vacancy_data),
                "employment_type": vacancy_data.get("employment", {}).get("name") or "–ù–µ —É–∫–∞–∑–∞–Ω",
                "schedule": vacancy_data.get("schedule", {}).get("name") or "–ù–µ —É–∫–∞–∑–∞–Ω",
                "experience_required": self.extract_experience_from_description(description,
                                                                                vacancy_data.get("experience")),
                "source_url": vacancy_data.get("alternate_url"),
                "source_name": "hh.ru",
                "publication_date": publication_date,
                "is_relevant": True,
                "company_id": company_id,
                "compensation_id": salary_info["id"],
                "benefits_id": benefits["id"],
                "created_at": datetime.now().isoformat()
            },
            "companies": {
                "id": company_id,
                "name": company_name,
                "name_variations": name_variations,
                "industry": "–ù–µ —É–∫–∞–∑–∞–Ω",
                "size": self.determine_company_size(employer_data),
                "is_foreign": False,
                "location_city": vacancy_data.get("area", {}).get("name") or "–ù–µ —É–∫–∞–∑–∞–Ω",
                "location_radius_km": 50
            },
            "compensations": salary_info,
            "benefits": benefits,
        }

    def detect_work_format(self, vacancy_data: Dict) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ñ–æ—Ä–º–∞—Ç —Ä–∞–±–æ—Ç—ã"""
        schedule = vacancy_data.get("schedule", {}).get("name", "").lower()
        description = vacancy_data.get("description", "").lower()
        if "—É–¥–∞–ª–µ–Ω" in schedule or "remote" in schedule or "—É–¥–∞–ª–µ–Ω–Ω–∞—è —Ä–∞–±–æ—Ç–∞" in description:
            work_format = "remote"
        elif "–≥–∏–±—Ä–∏–¥" in schedule or "–≥–∏–±—Ä–∏–¥–Ω—ã–π" in description:
            work_format = "hybrid"
        else:
            work_format = "office"
        self.logger.debug(f"–û–ø—Ä–µ–¥–µ–ª–µ–Ω —Ñ–æ—Ä–º–∞—Ç —Ä–∞–±–æ—Ç—ã: {work_format}")
        return work_format

    def save_chunk(self, chunk_data: List[Dict], chunk_number: int) -> bool:
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —á–∞–Ω–∫ –¥–∞–Ω–Ω—ã—Ö –≤ JSON —Ñ–∞–π–ª"""
        if not self.output_dir:
            self.logger.warning("–ü–∞–ø–∫–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")
            return False

        filename = self.output_dir / f"hh_vacancies_part{chunk_number}.json"
        try:
            with open(filename, "w", encoding="utf-8") as f:
                json.dump(chunk_data, f, ensure_ascii=False, indent=2)
            self.saved_files.append(str(filename))
            self.logger.info(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω —Ñ–∞–π–ª {filename} —Å {len(chunk_data)} –≤–∞–∫–∞–Ω—Å–∏—è–º–∏")
            return True
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞ {filename}: {e}")
            return False

    def process_and_send(self, vacancy: Dict):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤–∞–∫–∞–Ω—Å–∏—é –∏ –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –≤ RabbitMQ –ø—Ä–∏ –Ω–∞–∫–æ–ø–ª–µ–Ω–∏–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞"""
        parsed = self.parse_vacancy(vacancy, require_salary=True)  # –î–æ–±–∞–≤–ª–µ–Ω –ø–∞—Ä–∞–º–µ—Ç—Ä require_salary
        if parsed:
            self.parsed_data.append(parsed)

            if len(self.parsed_data) >= self.chunk_size:
                self.send_to_rabbitmq(self.parsed_data)
                self.save_chunk(self.parsed_data, len(self.saved_files) + 1)
                self.parsed_data = []

    def final_send(self):
        """–û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –æ—Å—Ç–∞–≤—à–∏–µ—Å—è –¥–∞–Ω–Ω—ã–µ"""
        if self.parsed_data:
            self.send_to_rabbitmq(self.parsed_data)
            self.save_chunk(self.parsed_data, len(self.saved_files) + 1)
            self.parsed_data = []

    def load_job_titles(self, file_path: str) -> List[str]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –Ω–∞–∑–≤–∞–Ω–∏–π –≤–∞–∫–∞–Ω—Å–∏–π –∏–∑ JSON —Ñ–∞–π–ª–∞"""
        try:
            full_path = Path(__file__).parent.parent.parent / file_path
            with open(full_path, "r", encoding="utf-8") as f:
                titles = json.load(f)
            self.logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(titles)} –Ω–∞–∑–≤–∞–Ω–∏–π –≤–∞–∫–∞–Ω—Å–∏–π –∏–∑ {file_path}")
            return titles
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Ñ–∞–π–ª–∞ {file_path}: {e}")
            return []

    def run_parsing(self, input_file: str) -> bool:
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –∑–∞–ø—É—Å–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞"""
        if not self.connect_rabbitmq():
            self.logger.error("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ RabbitMQ")
            return False

        job_titles = self.load_job_titles(input_file)
        if not job_titles:
            self.logger.error("–ù–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –Ω–∞–∑–≤–∞–Ω–∏—è –≤–∞–∫–∞–Ω—Å–∏–π")
            return False

        time_str = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.init_output_dir(time_str)

        try:
            for title in job_titles:
                self.logger.info(f"–ü–æ–∏—Å–∫ –≤–∞–∫–∞–Ω—Å–∏–π –¥–ª—è: {title}")
                params = {"text": title, "area": 1, "per_page": 5}

                vacancies = self.fetch_vacancies(params)
                for vacancy in vacancies:
                    time.sleep(0.5)
                    full_data = self.get_vacancy_details(vacancy["id"])
                    if full_data:
                        self.process_and_send(full_data)

            self.final_send()
            return True
        except KeyboardInterrupt:
            self.logger.info("–ü–∞—Ä—Å–∏–Ω–≥ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
            self.final_send()
            return False
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: {e}")
            return False
        finally:
            self.close_rabbitmq()